{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7af2941-4ddc-4ef8-b060-cd4c094cf887",
   "metadata": {},
   "source": [
    "# Data Imputation with MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3adc2b68-3898-405f-89a0-7002e02dd49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge # A common estimator for IterativeImputer\n",
    "import h5py\n",
    "import os \n",
    "import math \n",
    "from scipy.spatial import cKDTree\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.feature as cfeature\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import griddata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac971e03-8330-426b-86b3-00a1e3956c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the grid points\n",
    "def generate_grid_points(top_left_lat, top_left_lon, bottom_right_lat, bottom_right_lon, grid_size):\n",
    "    grid_points = []\n",
    "\n",
    "    # Calculate the distance between the top-left and bottom-right corners\n",
    "    lat_distance = abs(top_left_lat - bottom_right_lat)\n",
    "    lon_distance = abs(top_left_lon - bottom_right_lon)\n",
    "\n",
    "    # Calculate the number of grids in latitude and longitude directions\n",
    "    num_lat_grids = int(lat_distance * 111.32 / grid_size)  # 1 degree latitude ~ 111.32 km\n",
    "    num_lon_grids = int(lon_distance * 111.32 * math.cos(math.radians(top_left_lat)) / grid_size)\n",
    "\n",
    "    # Generate grid points\n",
    "    for i in range(num_lat_grids + 1):\n",
    "        for j in range(num_lon_grids + 1):\n",
    "            lat = top_left_lat - (i * grid_size / 111.32)\n",
    "            lon = top_left_lon + (j * grid_size / (111.32 * math.cos(math.radians(top_left_lat))))\n",
    "            grid_points.append((lat, lon))\n",
    "\n",
    "    return grid_points\n",
    "\n",
    "def list_files(directory: str, ftype):\n",
    "    \"\"\"\n",
    "    List files all file in given folder.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Directory to search for files.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are week ranges and values are lists of matching files.\n",
    "    \"\"\"\n",
    "    matching_files = []\n",
    "    matching_files.extend(\n",
    "        [directory+\"/\"+f for f in os.listdir(directory) if f.endswith(ftype)]\n",
    "    )\n",
    "    #files_by_week[f\"{start} to {stop}\"] = matching_files\n",
    "\n",
    "    return matching_files\n",
    "\n",
    "def load_combined_file(file_path):\n",
    "    \"\"\"\n",
    "    Load the combined HDF5 file and extract the data.\n",
    "    \"\"\"\n",
    "    print(file_path)\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        soil_moisture = f['soil_moisture'][:]\n",
    "        latitude = f['latitude'][:]\n",
    "        longitude = f['longitude'][:]\n",
    "    return soil_moisture, latitude, longitude\n",
    "\n",
    "def genSMAP(filtered_stations, smap_locations, _smapDf, paraName):\n",
    "    tele_locations = filtered_stations[['latitude', 'longitude']].to_numpy()\n",
    "    tele_values = filtered_stations['val'].to_numpy()\n",
    "    \n",
    "    #print(tele_locations, tele_values)\n",
    "    \n",
    "    smap_tree = cKDTree(smap_locations)\n",
    "    \n",
    "    # Keep track of used locations in smapDf\n",
    "    used_smap_indices = set()\n",
    "    \n",
    "    # Prepare a column to store results\n",
    "    _smapDf[paraName] = np.nan  # New column for matched SMAP values\n",
    "    \n",
    "    \n",
    "    # Iterate over each smap location and match it to the nearest tele location\n",
    "    for idx, tele_loc in enumerate(tele_locations):\n",
    "        # Query the nearest tele location\n",
    "        #distance, tele_idx = tele_tree.query(tele_loc)\n",
    "        distance, smap_idx = smap_tree.query(tele_loc)\n",
    "        #print(distance, smap_idx,idx , tele_loc, smap_locations[smap_idx])\n",
    "    \n",
    "        if smap_idx not in used_smap_indices:\n",
    "            _smapDf.loc[smap_idx, paraName] = tele_values[idx]\n",
    "            #print(distance, smap_idx,idx , tele_loc, smap_locations[smap_idx],tele_values[idx], smapDf['matched_smap_val'][idx])\n",
    "            used_smap_indices.add(smap_idx)  # Mark this SMAP index as used\n",
    "    \n",
    "    #print(smapDf)\n",
    "    return _smapDf\n",
    "\n",
    "def ReadData(csv_file, lat, lon):\n",
    "    # Determine the bounding box of the SMAP data\n",
    "    lat_min, lat_max = min(lat), max(lat)\n",
    "    lon_min, lon_max = min(lon), max(lon)\n",
    "\n",
    "    # Load telemetry station data (CSV format assumed)\n",
    "    #tele_data = pd.read_csv(csv_file, names=['code','latitude','longitude','val'])\n",
    "    tele_data = pd.read_csv(csv_file)\n",
    "    #print(tele_data)\n",
    "\n",
    "    # Filter telemetry stations within SMAP bounding box\n",
    "    filtered_stations = tele_data[\n",
    "        (tele_data['latitude'] >= lat_min) & (tele_data['latitude'] <= lat_max) &\n",
    "        (tele_data['longitude'] >= lon_min) & (tele_data['longitude'] <= lon_max)\n",
    "    ]\n",
    "    return filtered_stations\n",
    "# Convert dataframes into structured grid format\n",
    "def prepare_data(dfs):\n",
    "    X = []\n",
    "    #for _df in dfs:\n",
    "        \n",
    "    #val_grid = interpolate_feature(dfs, 'val')\n",
    "    \n",
    "    # Applying IDW to interpolate rain data on grid\n",
    "    humid_grid = inverse_distance_weighting(\n",
    "        dfs['latitude'].values,\n",
    "        dfs['longitude'].values,\n",
    "        dfs['humid'].values,\n",
    "        lat_grid,\n",
    "        lon_grid\n",
    "    )\n",
    "    \n",
    "    #dfs['humid_idw'] = humid_grid\n",
    "    \n",
    "    rain_grid = inverse_distance_weighting(\n",
    "        dfs['latitude'].values,\n",
    "        dfs['longitude'].values,\n",
    "        dfs['rain'].values,\n",
    "        lat_grid,\n",
    "        lon_grid\n",
    "    )\n",
    "    temp_grid = inverse_distance_weighting(\n",
    "        dfs['latitude'].values,\n",
    "        dfs['longitude'].values,\n",
    "        dfs['temp'].values,\n",
    "        lat_grid,\n",
    "        lon_grid\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    input_data = np.stack([humid_grid, rain_grid, temp_grid], axis=-1)\n",
    "    input_data = np.nan_to_num(input_data, nan=0)  # Replace NaNs with 0 for training\n",
    "    X.append(input_data)\n",
    "\n",
    "    # Stack into a single 3D array\n",
    "    y_input_data = np.stack([val_grid], axis=-1)\n",
    "    y_input_data = np.nan_to_num(y_input_data, nan=0)  # Replace NaNs with 0 for training\n",
    "    Y.append(y_input_data)\n",
    "    input_data = np.stack([humid_grid, rain_grid, temp_grid], axis=-1)\n",
    "    input_data = np.nan_to_num(input_data, nan=0)  # Replace NaNs with 0 for training\n",
    "    #X.append(input_data)  //generate Array\n",
    "    \"\"\"\n",
    "    \n",
    "    return humid_grid, rain_grid, temp_grid\n",
    "\n",
    "# Function to interpolate missing values\n",
    "def interpolate_feature(_df, feature):\n",
    "    known_points = _df[['latitude', 'longitude']][_df[feature].notna()].values\n",
    "    known_values = _df[feature].dropna().values\n",
    "    grid_values = griddata(known_points, known_values, (grid_lat, grid_lon), method='cubic')\n",
    "    return grid_values\n",
    "\n",
    "# IDW Interpolation function\n",
    "def inverse_distance_weighting(x, y, values, xi, yi, power=2):\n",
    "    tree = cKDTree(np.c_[x, y])\n",
    "    distances, indices = tree.query(np.c_[xi.ravel(), yi.ravel()], k=5)\n",
    "    weights = 1 / distances ** power\n",
    "    weighted_values = np.sum(weights * values[indices], axis=1) / np.sum(weights, axis=1)\n",
    "    print(weighted_values)\n",
    "    return weighted_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54f67a34-7529-44bc-813f-0fedbe81ebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         latitude   longitude\n",
      "0       18.607933  101.005346\n",
      "1       18.607933  101.014825\n",
      "2       18.607933  101.024303\n",
      "3       18.607933  101.033782\n",
      "4       18.607933  101.043260\n",
      "...           ...         ...\n",
      "269819  14.017563  105.953182\n",
      "269820  14.017563  105.962661\n",
      "269821  14.017563  105.972139\n",
      "269822  14.017563  105.981618\n",
      "269823  14.017563  105.991097\n",
      "\n",
      "[269824 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "EARTH_RADIUS = 6371  # Earth's radius in kilometers\n",
    "grid_size = 1\n",
    "\n",
    "# NorthEast\n",
    "lat_range = [18.607933, 14.012681]  # Define the latitude range of interest\n",
    "lon_range = [101.005346, 105.995516]  # Define the longitude range of interest\n",
    "\n",
    "grid_points = generate_grid_points(lat_range[0], lon_range[0], lat_range[1], lon_range[1], grid_size)\n",
    "\n",
    "points = pd.DataFrame(grid_points, columns=['latitude', 'longitude'])\n",
    "print(points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0849a19-4c6e-4091-a748-eb4562841304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-01-02to2023-01-08.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-01-09to2023-01-15.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-01-16to2023-01-22.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-01-23to2023-01-29.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-01-30to2023-02-05.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-02-06to2023-02-12.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-02-13to2023-02-19.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-02-20to2023-02-26.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-02-27to2023-03-05.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-03-06to2023-03-12.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-03-13to2023-03-19.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-03-20to2023-03-26.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-03-27to2023-04-02.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-04-03to2023-04-09.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-04-10to2023-04-16.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-04-17to2023-04-23.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-04-24to2023-04-30.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-05-01to2023-05-07.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-05-08to2023-05-14.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-05-15to2023-05-21.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-05-22to2023-05-28.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-05-29to2023-06-04.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-06-05to2023-06-11.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-06-12to2023-06-18.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-06-19to2023-06-25.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-06-26to2023-07-02.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-07-03to2023-07-09.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-07-10to2023-07-16.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-07-17to2023-07-23.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-07-24to2023-07-30.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-07-31to2023-08-06.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-08-07to2023-08-13.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-08-14to2023-08-20.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-08-21to2023-08-27.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-08-28to2023-09-03.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-09-04to2023-09-10.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-09-11to2023-09-17.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-09-18to2023-09-24.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-09-25to2023-10-01.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-10-02to2023-10-08.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-10-09to2023-10-15.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-10-16to2023-10-22.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-10-23to2023-10-29.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-10-30to2023-11-05.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-11-06to2023-11-12.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-11-13to2023-11-19.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-11-20to2023-11-26.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-11-27to2023-12-03.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-12-04to2023-12-10.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-12-11to2023-12-17.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-12-18to2023-12-24.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2023-12-25to2023-12-31.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-01-01to2024-01-07.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-01-08to2024-01-14.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-01-15to2024-01-21.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-01-22to2024-01-28.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-01-29to2024-02-04.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-02-05to2024-02-11.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-02-12to2024-02-18.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-02-19to2024-02-25.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-02-26to2024-03-03.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-03-04to2024-03-10.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-03-11to2024-03-17.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-03-18to2024-03-24.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-03-25to2024-03-31.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-04-01to2024-04-07.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-04-08to2024-04-14.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-04-15to2024-04-21.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-04-22to2024-04-28.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-04-29to2024-05-05.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-05-06to2024-05-12.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-05-13to2024-05-19.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-05-20to2024-05-26.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-05-27to2024-06-02.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-06-03to2024-06-09.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-06-10to2024-06-16.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-06-17to2024-06-23.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-06-24to2024-06-30.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-07-01to2024-07-07.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-07-08to2024-07-14.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-07-15to2024-07-21.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-07-22to2024-07-28.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-07-29to2024-08-04.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-08-05to2024-08-11.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-08-12to2024-08-18.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-08-19to2024-08-25.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-08-26to2024-09-01.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-09-02to2024-09-08.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-09-09to2024-09-15.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-09-16to2024-09-22.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-09-23to2024-09-29.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-09-30to2024-10-06.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-10-07to2024-10-13.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-10-14to2024-10-20.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-10-21to2024-10-27.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-10-28to2024-11-03.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-11-04to2024-11-10.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-11-11to2024-11-17.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-11-18to2024-11-24.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-11-25to2024-12-01.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-12-02to2024-12-08.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-12-09to2024-12-15.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-12-16to2024-12-22.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-12-23to2024-12-29.h5', 'C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\/2024-12-30to2024-12-31.h5']\n"
     ]
    }
   ],
   "source": [
    "# Paths to your data files\n",
    "smap_dir = \"C:\\\\Work\\\\Github\\\\SMAP\\\\Weekly\\\\Thailand\\\\\"  # Replace with your .h5 file\n",
    "\n",
    "tele_humid_dir = 'C:\\\\Work\\\\Github\\\\SMAP\\\\AvgTeleHumid\\\\AvgTeleHumid_'  # Replace with your telemetry data file\n",
    "tele_temp_dir = 'C:\\\\Work\\\\Github\\\\SMAP\\\\AvgTeleTemp\\\\AvgTeleTemp_'  # Replace with your telemetry data file\n",
    "tele_rain_dir = 'C:\\\\Work\\\\Github\\\\SMAP\\\\AvgTeleRain\\\\AvgTeleRain_'  # Replace with your telemetry data file\n",
    "\n",
    "h5_files = list_files(smap_dir,'.h5')\n",
    "print(h5_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d84706-6b16-4b23-b113-cc9b1968effb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269824\n",
      "C:\\Work\\Github\\SMAP\\Weekly\\Thailand\\/2023-01-02to2023-01-08.h5\n"
     ]
    }
   ],
   "source": [
    "allDf = []\n",
    "TrainList = []\n",
    "TestList = []\n",
    "cc = 0\n",
    "\n",
    "# Build a KDTree for smapDf locations\n",
    "smap_locations = points[['latitude', 'longitude']].to_numpy()\n",
    "print(len(smap_locations))\n",
    "\n",
    "lenLat = len(points['latitude'].unique())\n",
    "lenLon = len(points['longitude'].unique())\n",
    "\n",
    "\n",
    "#lat_grid = np.linspace(points['latitude'].min(), points['latitude'].max(), lenLat)\n",
    "#lon_grid = np.linspace(points['longitude'].min(), points['longitude'].max(), lenLon)\n",
    "#grid_lat, grid_lon = np.meshgrid(lat_grid, lon_grid)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "first_round = True\n",
    "\n",
    "for combined_file in h5_files:\n",
    "    #print(combined_file)\n",
    "    soil_moisture, latitude, longitude = load_combined_file(combined_file)\n",
    "    soil_moisture = np.ma.masked_invalid(soil_moisture)\n",
    "    p_name = combined_file.replace(smap_dir+\"/\",\"\")\n",
    "    p_name = p_name.replace(\".h5\",\"\")\n",
    "    p_name = p_name[:10].replace(\"-\",\"\")\n",
    "    #print(p_name)\n",
    "\n",
    "    humidDf= ReadData(tele_humid_dir+p_name+\".csv\", latitude, longitude)\n",
    "    humidDf = humidDf.reset_index(drop=True)\n",
    "    #print(humidDf)\n",
    "\n",
    "    tempDf= ReadData(tele_temp_dir+p_name+\".csv\", latitude, longitude)\n",
    "    tempDf = tempDf.reset_index(drop=True)\n",
    "    #print(tempDf)\n",
    "\n",
    "    rainDf= ReadData(tele_rain_dir+p_name+\".csv\", latitude, longitude)\n",
    "    rainDf = rainDf.reset_index(drop=True)\n",
    "    #print(rainDf)\n",
    "    \n",
    "    smapDf = pd.DataFrame(columns=['latitude','longitude','val'])\n",
    "    smapDf['latitude'] = latitude\n",
    "    smapDf['longitude'] = longitude\n",
    "    smapDf['val'] = soil_moisture\n",
    "    \n",
    "    #print(smapDf[:10])\n",
    "    \n",
    "    smapDf = genSMAP(smapDf, smap_locations, points, \"val\")\n",
    "\n",
    "    #break\n",
    "    smapDf = genSMAP(humidDf, smap_locations, smapDf, \"humid\")\n",
    "    #print(smapDf[:10])\n",
    "    smapDf = genSMAP(rainDf, smap_locations, smapDf, \"rain\")\n",
    "    #print(smapDf[:10])\n",
    "    smapDf = genSMAP(tempDf, smap_locations, smapDf, \"temp\")\n",
    "\n",
    "    if first_round:\n",
    "        smapDf[['val','humid','rain','temp']] = min_max_scaler.fit_transform(smapDf[['val','humid','rain','temp']])\n",
    "        first_round = False\n",
    "    else:\n",
    "        smapDf[['val','humid','rain','temp']] = min_max_scaler.transform(smapDf[['val','humid','rain','temp']])\n",
    "\n",
    "    #allDf.append(smapDf.copy())\n",
    "    \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5509a998-f04a-48e0-b27a-632c2e0092fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
