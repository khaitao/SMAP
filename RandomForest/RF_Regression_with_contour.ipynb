{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926bdb62",
   "metadata": {},
   "source": [
    "# RandomForest with Contour. We using contour technique to generate the contour map of telemetry data which can be represent the data in missing area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa319749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os \n",
    "import math\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.spatial import cKDTree\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.feature as cfeature\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d388c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframes into structured grid format\n",
    "def prepare_data(dfs):\n",
    "    X = []\n",
    "    Y = []\n",
    "    #for _df in dfs:\n",
    "        \n",
    "    val_grid = interpolate_feature(dfs, 'val')\n",
    "    \n",
    "    # Applying IDW to interpolate rain data on grid\n",
    "    humid_grid = inverse_distance_weighting(\n",
    "        dfs['latitude'].values,\n",
    "        dfs['longitude'].values,\n",
    "        dfs['humid'].values,\n",
    "        lat_grid,\n",
    "        lon_grid\n",
    "    )\n",
    "    \n",
    "    dfs['humid_idw'] = humid_grid\n",
    "    \n",
    "    rain_grid = inverse_distance_weighting(\n",
    "        dfs['latitude'].values,\n",
    "        dfs['longitude'].values,\n",
    "        dfs['rain'].values,\n",
    "        lat_grid,\n",
    "        lon_grid\n",
    "    )\n",
    "    temp_grid = inverse_distance_weighting(\n",
    "        dfs['latitude'].values,\n",
    "        dfs['longitude'].values,\n",
    "        dfs['temp'].values,\n",
    "        lat_grid,\n",
    "        lon_grid\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    input_data = np.stack([humid_grid, rain_grid, temp_grid], axis=-1)\n",
    "    input_data = np.nan_to_num(input_data, nan=0)  # Replace NaNs with 0 for training\n",
    "    X.append(input_data)\n",
    "\n",
    "    # Stack into a single 3D array\n",
    "    y_input_data = np.stack([val_grid], axis=-1)\n",
    "    y_input_data = np.nan_to_num(y_input_data, nan=0)  # Replace NaNs with 0 for training\n",
    "    Y.append(y_input_data)\n",
    "    \n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def calculate_new_lat_lon(lat, lon, distance, bearing):\n",
    "    lat_rad = math.radians(lat)\n",
    "    lon_rad = math.radians(lon)\n",
    "    angular_distance = distance / EARTH_RADIUS\n",
    "\n",
    "    new_lat = math.asin(math.sin(lat_rad) * math.cos(angular_distance) +\n",
    "                        math.cos(lat_rad) * math.sin(angular_distance) * math.cos(bearing))\n",
    "\n",
    "    new_lon = lon_rad + math.atan2(math.sin(bearing) * math.sin(angular_distance) * math.cos(lat_rad),\n",
    "                                   math.cos(angular_distance) - math.sin(lat_rad) * math.sin(new_lat))\n",
    "\n",
    "    new_lat = math.degrees(new_lat)\n",
    "    new_lon = math.degrees(new_lon)\n",
    "\n",
    "    return new_lat, new_lon\n",
    "\n",
    "# Function to generate the grid points\n",
    "def generate_grid_points(top_left_lat, top_left_lon, bottom_right_lat, bottom_right_lon, grid_size):\n",
    "    grid_points = []\n",
    "\n",
    "    # Calculate the distance between the top-left and bottom-right corners\n",
    "    lat_distance = abs(top_left_lat - bottom_right_lat)\n",
    "    lon_distance = abs(top_left_lon - bottom_right_lon)\n",
    "\n",
    "    # Calculate the number of grids in latitude and longitude directions\n",
    "    num_lat_grids = int(lat_distance * 111.32 / grid_size)  # 1 degree latitude ~ 111.32 km\n",
    "    num_lon_grids = int(lon_distance * 111.32 * math.cos(math.radians(top_left_lat)) / grid_size)\n",
    "\n",
    "    # Generate grid points\n",
    "    for i in range(num_lat_grids + 1):\n",
    "        for j in range(num_lon_grids + 1):\n",
    "            lat = top_left_lat - (i * grid_size / 111.32)\n",
    "            lon = top_left_lon + (j * grid_size / (111.32 * math.cos(math.radians(top_left_lat))))\n",
    "            grid_points.append((lat, lon))\n",
    "\n",
    "    return grid_points\n",
    "\n",
    "def ReadData(csv_file, lat, lon):\n",
    "    # Determine the bounding box of the SMAP data\n",
    "    lat_min, lat_max = min(lat), max(lat)\n",
    "    lon_min, lon_max = min(lon), max(lon)\n",
    "\n",
    "    # Load telemetry station data (CSV format assumed)\n",
    "    #tele_data = pd.read_csv(csv_file, names=['code','latitude','longitude','val'])\n",
    "    tele_data = pd.read_csv(csv_file)\n",
    "    #print(tele_data)\n",
    "\n",
    "    # Filter telemetry stations within SMAP bounding box\n",
    "    filtered_stations = tele_data[\n",
    "        (tele_data['latitude'] >= lat_min) & (tele_data['latitude'] <= lat_max) &\n",
    "        (tele_data['longitude'] >= lon_min) & (tele_data['longitude'] <= lon_max)\n",
    "    ]\n",
    "    return filtered_stations\n",
    "\n",
    "def genSMAP(filtered_stations, smap_locations, _smapDf, paraName):\n",
    "    tele_locations = filtered_stations[['latitude', 'longitude']].to_numpy()\n",
    "    tele_values = filtered_stations['val'].to_numpy()\n",
    "\n",
    "    #print(tele_locations, tele_values)\n",
    "\n",
    "    smap_tree = cKDTree(smap_locations)\n",
    "\n",
    "    # Keep track of used locations in smapDf\n",
    "    used_smap_indices = set()\n",
    "\n",
    "    # Prepare a column to store results\n",
    "    _smapDf[paraName] = np.nan  # New column for matched SMAP values\n",
    "\n",
    "\n",
    "    # Iterate over each smap location and match it to the nearest tele location\n",
    "    for idx, tele_loc in enumerate(tele_locations):\n",
    "        # Query the nearest tele location\n",
    "        #distance, tele_idx = tele_tree.query(tele_loc)\n",
    "        distance, smap_idx = smap_tree.query(tele_loc)\n",
    "        #print(distance, smap_idx,idx , tele_loc, smap_locations[smap_idx])\n",
    "\n",
    "        if smap_idx not in used_smap_indices:\n",
    "            _smapDf.loc[smap_idx, paraName] = tele_values[idx]\n",
    "            #print(distance, smap_idx,idx , tele_loc, smap_locations[smap_idx],tele_values[idx], smapDf['matched_smap_val'][idx])\n",
    "            used_smap_indices.add(smap_idx)  # Mark this SMAP index as used\n",
    "\n",
    "    #print(smapDf)\n",
    "    return _smapDf\n",
    "\n",
    "# Function to interpolate missing values\n",
    "def interpolate_feature(_df, feature):\n",
    "    known_points = _df[['latitude', 'longitude']][_df[feature].notna()].values\n",
    "    known_values = _df[feature].dropna().values\n",
    "    grid_values = griddata(known_points, known_values, (grid_lat, grid_lon), method='cubic')\n",
    "    return grid_values\n",
    "\n",
    "def list_files(directory: str, ftype):\n",
    "    \"\"\"\n",
    "    List files all file in given folder.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Directory to search for files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are week ranges and values are lists of matching files.\n",
    "    \"\"\"\n",
    "    matching_files = []\n",
    "    matching_files.extend(\n",
    "        [directory+\"/\"+f for f in os.listdir(directory) if f.endswith(ftype)]\n",
    "    )\n",
    "    #files_by_week[f\"{start} to {stop}\"] = matching_files\n",
    "\n",
    "    return matching_files\n",
    "\n",
    "def load_combined_file(file_path):\n",
    "    \"\"\"\n",
    "    Load the combined HDF5 file and extract the data.\n",
    "    \"\"\"\n",
    "    print(file_path)\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        soil_moisture = f['soil_moisture'][:]\n",
    "        latitude = f['latitude'][:]\n",
    "        longitude = f['longitude'][:]\n",
    "\n",
    "    #print(soil_moisture)\n",
    "\n",
    "    \"\"\"\n",
    "    plot_on_map(\n",
    "        latitude,\n",
    "        longitude,\n",
    "        soil_moisture,\n",
    "        [4.5, 25.5, 95.5, 110.5],\n",
    "        #p_name,\n",
    "        title=\"\"\n",
    "    )\n",
    "    \"\"\"\n",
    "    return soil_moisture, latitude, longitude\n",
    "\n",
    "def generate_points_in_boundary(start_lat, start_lon, end_lat, end_lon, interval_km=1):\n",
    "    \"\"\"\n",
    "    Generate latitude and longitude points at every `interval_km` between two points.\n",
    "    \"\"\"\n",
    "    # Define the starting and ending points\n",
    "    start_point = Point(start_lat, start_lon)\n",
    "    end_point = Point(end_lat, end_lon)\n",
    "\n",
    "    # Calculate the total distance between the start and end points\n",
    "    total_distance = geodesic(start_point, end_point).kilometers\n",
    "\n",
    "    # Calculate the bearing (direction) from start to end\n",
    "    bearing = calculate_bearing(start_point, end_point)\n",
    "\n",
    "    # Generate points along the line at 1 km intervals\n",
    "    pointsDf = pd.DataFrame(columns=['latitude','longitude'])\n",
    "    for km in range(0, int(total_distance), interval_km):\n",
    "        # Calculate the new point at the given distance and bearing\n",
    "        new_point = distance(kilometers=km).destination(point=start_point, bearing=bearing)\n",
    "        pointsDf = pointsDf._append({'latitude': new_point.latitude,\n",
    "                       'longitude': new_point.longitude},\n",
    "                      ignore_index=True)\n",
    "        #points.append((new_point.latitude, new_point.longitude))\n",
    "\n",
    "    return pointsDf\n",
    "\n",
    "def calculate_bearing(start_point, end_point):\n",
    "    \"\"\"\n",
    "    Calculate the bearing (direction) from start_point to end_point.\n",
    "    \"\"\"\n",
    "    lat1 = math.radians(start_point.latitude)\n",
    "    lon1 = math.radians(start_point.longitude)\n",
    "    lat2 = math.radians(end_point.latitude)\n",
    "    lon2 = math.radians(end_point.longitude)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    x = math.sin(dlon) * math.cos(lat2)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(dlon)\n",
    "    bearing = math.atan2(x, y)\n",
    "    bearing = math.degrees(bearing)\n",
    "    bearing = (bearing + 360) % 360  # Normalize to 0-360 degrees\n",
    "    return bearing\n",
    "\n",
    "def plot_on_map(latitude, longitude, soil_moisture, region_bounds, title):\n",
    "    \"\"\"\n",
    "    Plot soil moisture data on a map using cartopy.\n",
    "    \"\"\"\n",
    "    # Create the map projection\n",
    "    projection = ccrs.PlateCarree()\n",
    "\n",
    "    # Create the figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw={'projection': projection})\n",
    "\n",
    "    # Set the map extent to Thailand\n",
    "    min_lat, max_lat, min_lon, max_lon = region_bounds\n",
    "    ax.set_extent([min_lon, max_lon, min_lat, max_lat], crs=projection)\n",
    "\n",
    "    # Add map features\n",
    "    ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "\n",
    "    sc = ax.scatter(longitude, latitude, c=soil_moisture, cmap='YlGnBu', marker='s', s=5, transform=projection)\n",
    "    plt.colorbar(sc, ax=ax, orientation='vertical', label='Soil Moisture')\n",
    "\n",
    "    # Add a title\n",
    "    ax.set_title(title, fontsize=14)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# IDW Interpolation function\n",
    "def inverse_distance_weighting(x, y, values, xi, yi, power=2):\n",
    "    tree = cKDTree(np.c_[x, y])\n",
    "    distances, indices = tree.query(np.c_[xi.ravel(), yi.ravel()], k=5)\n",
    "    weights = 1 / distances ** power\n",
    "    weighted_values = np.sum(weights * values[indices], axis=1) / np.sum(weights, axis=1)\n",
    "    print(weighted_values)\n",
    "    return weighted_values\n",
    "    #return weighted_values.reshape(xi.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a9097e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2  Target\n",
      "0       1.0      10.0   100.0\n",
      "1       2.0      15.0   200.0\n",
      "2       3.0       NaN   300.0\n",
      "4       NaN      25.0   500.0\n",
      "   Feature1  Feature2  Target\n",
      "3       4.0      20.0     NaN\n",
      "Index(['Feature1', 'Feature2'], dtype='object')\n",
      "   Feature1  Feature2  Target\n",
      "0       1.0      10.0   100.0\n",
      "1       2.0      15.0   200.0\n",
      "   Feature1  Feature2  Target\n",
      "0       1.0      10.0   100.0\n",
      "1       2.0      15.0   200.0\n",
      "2       3.0       NaN   300.0\n",
      "3       4.0      20.0   174.0\n",
      "4       NaN      25.0   500.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample data with missing values\n",
    "data = {\n",
    "    'Feature1': [1.0, 2.0, 3.0, 4.0, np.nan],\n",
    "    'Feature2': [10.0, 15.0, np.nan, 20.0, 25.0],\n",
    "    'Target':   [100.0, 200.0, 300.0, np.nan, 500.0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Choose the column you want to impute (e.g., 'Target')\n",
    "target_col = 'Target'\n",
    "\n",
    "# Split data into rows with and without missing values in target\n",
    "df_train_ori = df[df[target_col].notnull()].copy()\n",
    "df_missing = df[df[target_col].isnull()].copy()\n",
    "\n",
    "print(df_train_ori)\n",
    "print(df_missing)\n",
    "\n",
    "# Define features (excluding the target column)\n",
    "features = df.columns.drop(target_col)\n",
    "print(features)\n",
    "\n",
    "# Drop rows with missing values in the features from the training set\n",
    "df_train = df_train_ori.dropna(subset=features).copy()\n",
    "print(df_train)\n",
    "\n",
    "\n",
    "# Train a RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(df_train[features], df_train[target_col])\n",
    "\n",
    "# Predict missing values\n",
    "df_missing = df_missing.copy()  # to avoid SettingWithCopyWarning\n",
    "df_missing[target_col] = model.predict(df_missing[features])\n",
    "\n",
    "# Combine the imputed rows with the original data\n",
    "df_imputed = pd.concat([df_train_ori, df_missing]).sort_index()\n",
    "\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6be3dfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         latitude   longitude\n",
      "0       18.607933  101.005346\n",
      "1       18.607933  101.014825\n",
      "2       18.607933  101.024303\n",
      "3       18.607933  101.033782\n",
      "4       18.607933  101.043260\n",
      "...           ...         ...\n",
      "269819  14.017563  105.953182\n",
      "269820  14.017563  105.962661\n",
      "269821  14.017563  105.972139\n",
      "269822  14.017563  105.981618\n",
      "269823  14.017563  105.991097\n",
      "\n",
      "[269824 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "EARTH_RADIUS = 6371  # Earth's radius in kilometers\n",
    "grid_size = 1\n",
    "\n",
    "# NorthEast\n",
    "lat_range = [18.607933, 14.012681]  # Define the latitude range of interest\n",
    "lon_range = [101.005346, 105.995516]  # Define the longitude range of interest\n",
    "\n",
    "# Top Left\n",
    "#lat_range = [18.607933, 16.310307]  # Define the latitude range of interest\n",
    "#lon_range = [101.005346, 103.5004]  # Define the longitude range of interest\n",
    "\n",
    "# Bottom Left\n",
    "#lat_range = [16.310307, 14.012681]  # Define the latitude range of interest\n",
    "#lon_range = [101.005346, 103.5004]  # Define the longitude range of interest\n",
    "\n",
    "# Top Right\n",
    "#lat_range = [18.607933, 16.310307]  # Define the latitude range of interest\n",
    "#lon_range = [103.5004, 105.995516]  # Define the longitude range of interest\n",
    "\n",
    "# Bottom Right\n",
    "#lat_range = [16.310307, 14.012681]  # Define the latitude range of interest\n",
    "#lon_range = [103.5004, 105.995516]  # Define the longitude range of interest\n",
    "\n",
    "# Calculate the distance in kilometers\n",
    "#distance_left_2_right = geodesic((lat_range[0], lon_range[0]), (lat_range[0], lon_range[1])).kilometers\n",
    "#distance_top_2_down = geodesic((lat_range[0], lon_range[0]), (lat_range[1], lon_range[0])).kilometers\n",
    "\n",
    "#print(distance_left_2_right)\n",
    "#print(distance_top_2_down)\n",
    "\n",
    "grid_points = generate_grid_points(lat_range[0], lon_range[0], lat_range[1], lon_range[1], grid_size)\n",
    "\n",
    "points = pd.DataFrame(grid_points, columns=['latitude', 'longitude'])\n",
    "print(points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "736c5f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-01-22to2024-01-28.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-11-13to2023-11-19.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-04-17to2023-04-23.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-12-11to2023-12-17.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-04-24to2023-04-30.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-08-05to2024-08-11.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-07-10to2023-07-16.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-03-18to2024-03-24.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-01-02to2023-01-08.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-10-14to2024-10-20.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-10-02to2023-10-08.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-11-04to2024-11-10.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-09-23to2024-09-29.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-01-15to2024-01-21.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-07-24to2023-07-30.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-11-18to2024-11-24.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-04-10to2023-04-16.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-01-08to2024-01-14.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-12-25to2023-12-31.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-05-20to2024-05-26.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-03-13to2023-03-19.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-10-30to2023-11-05.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-10-21to2024-10-27.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-07-17to2023-07-23.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-02-06to2023-02-12.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-05-01to2023-05-07.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-01-30to2023-02-05.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-03-04to2024-03-10.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-07-29to2024-08-04.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-08-19to2024-08-25.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-09-09to2024-09-15.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-10-28to2024-11-03.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-09-11to2023-09-17.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-09-30to2024-10-06.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-07-31to2023-08-06.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-02-05to2024-02-11.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-02-26to2024-03-03.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-07-03to2023-07-09.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-07-15to2024-07-21.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-12-23to2024-12-29.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-07-22to2024-07-28.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-06-12to2023-06-18.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-03-06to2023-03-12.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-04-08to2024-04-14.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-02-13to2023-02-19.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-05-29to2023-06-04.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-04-03to2023-04-09.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-04-15to2024-04-21.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-08-07to2023-08-13.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-01-16to2023-01-22.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-07-08to2024-07-14.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-02-19to2024-02-25.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-12-09to2024-12-15.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-10-16to2023-10-22.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-09-25to2023-10-01.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-11-06to2023-11-12.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-02-27to2023-03-05.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-05-27to2024-06-02.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-04-22to2024-04-28.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-04-01to2024-04-07.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-09-18to2023-09-24.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-08-21to2023-08-27.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-06-05to2023-06-11.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-12-02to2024-12-08.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-11-25to2024-12-01.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-01-29to2024-02-04.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-06-24to2024-06-30.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-11-20to2023-11-26.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-02-12to2024-02-18.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-06-17to2024-06-23.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-09-16to2024-09-22.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-07-01to2024-07-07.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-09-04to2023-09-10.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-08-14to2023-08-20.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-06-26to2023-07-02.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-03-20to2023-03-26.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-06-19to2023-06-25.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-05-13to2024-05-19.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-06-10to2024-06-16.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-04-29to2024-05-05.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-09-02to2024-09-08.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-10-23to2023-10-29.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-03-11to2024-03-17.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-05-06to2024-05-12.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-02-20to2023-02-26.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-10-07to2024-10-13.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-12-18to2023-12-24.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-01-23to2023-01-29.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-05-22to2023-05-28.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-03-27to2023-04-02.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-05-15to2023-05-21.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-08-26to2024-09-01.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-08-12to2024-08-18.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-12-30to2024-12-31.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-03-25to2024-03-31.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-11-11to2024-11-17.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-08-28to2023-09-03.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-05-08to2023-05-14.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-01-09to2023-01-15.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-12-16to2024-12-22.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-10-09to2023-10-15.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-11-27to2023-12-03.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-12-04to2023-12-10.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-01-01to2024-01-07.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2022-12-26to2023-01-01.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-06-03to2024-06-09.h5']\n"
     ]
    }
   ],
   "source": [
    "# Paths to your data files\n",
    "smap_dir = \"/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand/\"  # Replace with your .h5 file\n",
    "\n",
    "tele_humid_dir = '/Users/khaitao/Documents/GitHub/SMAP/AvgTeleHumid/AvgTeleHumid_'  # Replace with your telemetry data file\n",
    "tele_temp_dir = '/Users/khaitao/Documents/GitHub/SMAP/AvgTeleTemp/AvgTeleTemp_'  # Replace with your telemetry data file\n",
    "tele_rain_dir = '/Users/khaitao/Documents/GitHub/SMAP/AvgTeleRain/AvgTeleRain_'  # Replace with your telemetry data file\n",
    "\n",
    "h5_files = list_files(smap_dir,'.h5')\n",
    "#print(h5_files)\n",
    "\n",
    "\n",
    "h5_files = list_files(smap_dir,'.h5')\n",
    "print(h5_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9fd086b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269824\n",
      "512 527\n",
      "/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-01-22to2024-01-28.h5\n"
     ]
    }
   ],
   "source": [
    "allDf = []\n",
    "TrainList = []\n",
    "TestList = []\n",
    "cc = 0\n",
    "\n",
    "# Build a KDTree for smapDf locations\n",
    "smap_locations = points[['latitude', 'longitude']].to_numpy()\n",
    "print(len(smap_locations))\n",
    "\n",
    "#break\n",
    "\n",
    "lenLat = len(points['latitude'].unique())\n",
    "lenLon = len(points['longitude'].unique())\n",
    "\n",
    "print(lenLat, lenLon)\n",
    "#break\n",
    "\n",
    "#print(grid_lat.shape, grid_lon.shape)\n",
    "\n",
    "#break\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "first_round = True\n",
    "\n",
    "for combined_file in h5_files:\n",
    "    #print(combined_file)\n",
    "    soil_moisture, latitude, longitude = load_combined_file(combined_file)\n",
    "    #print(soil_moisture)\n",
    "\n",
    "    #print(latitude, longitude)\n",
    "    # Mask invalid soil moisture data\n",
    "    soil_moisture = np.ma.masked_invalid(soil_moisture)\n",
    "    p_name = combined_file.replace(smap_dir+\"/\",\"\")\n",
    "    p_name = p_name.replace(\".h5\",\"\")\n",
    "    p_name = p_name[:10].replace(\"-\",\"\")\n",
    "    #print(p_name)\n",
    "\n",
    "    humidDf= ReadData(tele_humid_dir+p_name+\".csv\", latitude, longitude)\n",
    "    humidDf = humidDf.reset_index(drop=True)\n",
    "    #print(humidDf)\n",
    "\n",
    "    tempDf= ReadData(tele_temp_dir+p_name+\".csv\", latitude, longitude)\n",
    "    tempDf = tempDf.reset_index(drop=True)\n",
    "    #print(tempDf)\n",
    "\n",
    "    rainDf= ReadData(tele_rain_dir+p_name+\".csv\", latitude, longitude)\n",
    "    rainDf = rainDf.reset_index(drop=True)\n",
    "    #print(rainDf)\n",
    "\n",
    "    smapDf = pd.DataFrame(columns=['latitude','longitude','val'])\n",
    "    smapDf['latitude'] = latitude\n",
    "    smapDf['longitude'] = longitude\n",
    "    smapDf['val'] = soil_moisture\n",
    "\n",
    "    #print(smapDf[:10])\n",
    "\n",
    "    smapDf = genSMAP(smapDf, smap_locations, points, \"val\")\n",
    "    #print(smapDf[:10])\n",
    "\n",
    "    #break\n",
    "    smapDf = genSMAP(humidDf, smap_locations, smapDf, \"humid\")\n",
    "    #print(smapDf[:10])\n",
    "    smapDf = genSMAP(rainDf, smap_locations, smapDf, \"rain\")\n",
    "    #print(smapDf[:10])\n",
    "    smapDf = genSMAP(tempDf, smap_locations, smapDf, \"temp\")\n",
    "    #print(smapDf[:10])\n",
    "    #print(len(smapDf))\n",
    "\n",
    "    if first_round:\n",
    "        smapDf[['val','humid','rain','temp']] = min_max_scaler.fit_transform(smapDf[['val','humid','rain','temp']])\n",
    "        first_round = False\n",
    "    else:\n",
    "        smapDf[['val','humid','rain','temp']] = min_max_scaler.transform(smapDf[['val','humid','rain','temp']])\n",
    "\n",
    "    allDf.append(smapDf.copy())\n",
    "\n",
    "    #print(f\"{cc}->{allDf}\")\n",
    "\n",
    "    #if cc<=5:\n",
    "    #    cc = cc+1\n",
    "    #else:\n",
    "    #   break\n",
    "\n",
    "    break\n",
    "\n",
    "#break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae5eb5e",
   "metadata": {},
   "source": [
    "# Contour Mapping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e734009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         latitude   longitude       val  humid  rain  temp\n",
      "158742  15.904016  102.095387       NaN    NaN   NaN   NaN\n",
      "180949  15.526726  102.787325       NaN    NaN   NaN   NaN\n",
      "181660  15.517743  104.531390  0.195524    NaN   NaN   NaN\n",
      "106988  16.784361  101.071696  0.000000    NaN   NaN   NaN\n",
      "186517  15.436895  105.611952       NaN    NaN   NaN   NaN\n",
      "...           ...         ...       ...    ...   ...   ...\n",
      "119879  16.568767  103.374999  0.541039    NaN   NaN   NaN\n",
      "259178  14.197225  104.995842  0.207577    NaN   NaN   NaN\n",
      "131932  16.362155  102.730454       NaN    NaN   NaN   NaN\n",
      "146867  16.110628  104.427125  0.091066    NaN   NaN   NaN\n",
      "121958  16.532834  103.100120  0.369621    NaN   NaN   NaN\n",
      "\n",
      "[188876 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Split into train (70) and test (30)\n",
    "train_dfs, test_dfs= train_test_split(allDf[0], test_size=0.3, random_state=42)\n",
    "\n",
    "print(train_dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4715d",
   "metadata": {},
   "source": [
    "# Create a grid for interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc71fcc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (527) does not match length of index (188876)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m grid_lat, grid_lon \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(lat_grid, lon_grid)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Prepare training and testing data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m X_train, Y_train \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m X_test, Y_test \u001b[38;5;241m=\u001b[39m prepare_data(test_dfs)\n",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(dfs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Applying IDW to interpolate rain data on grid\u001b[39;00m\n\u001b[1;32m     10\u001b[0m humid_grid \u001b[38;5;241m=\u001b[39m inverse_distance_weighting(\n\u001b[1;32m     11\u001b[0m     dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     12\u001b[0m     dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     lon_grid\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumid_idw\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m humid_grid\n\u001b[1;32m     20\u001b[0m rain_grid \u001b[38;5;241m=\u001b[39m inverse_distance_weighting(\n\u001b[1;32m     21\u001b[0m     dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     22\u001b[0m     dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     lon_grid\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m temp_grid \u001b[38;5;241m=\u001b[39m inverse_distance_weighting(\n\u001b[1;32m     28\u001b[0m     dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[1;32m     29\u001b[0m     dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     lon_grid\n\u001b[1;32m     33\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/universe_m1/lib/python3.9/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/universe_m1/lib/python3.9/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/miniforge3/envs/universe_m1/lib/python3.9/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/universe_m1/lib/python3.9/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (527) does not match length of index (188876)"
     ]
    }
   ],
   "source": [
    "\n",
    "lat_grid = np.linspace(points['latitude'].min(), points['latitude'].max(), max(lenLat,lenLon))\n",
    "lon_grid = np.linspace(points['longitude'].min(), points['longitude'].max(), max(lenLat,lenLon))\n",
    "#print(len(lat_grid), len(lon_grid))\n",
    "\n",
    "grid_lat, grid_lon = np.meshgrid(lat_grid, lon_grid)\n",
    "\n",
    "# Prepare training and testing data\n",
    "X_train, Y_train = prepare_data(train_dfs)\n",
    "\n",
    "X_test, Y_test = prepare_data(test_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d0427ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndf = allDf[0].copy()\\n\\n# Choose the column you want to impute (e.g., 'Target')\\ntarget_col = 'val'\\n\\n# Split data into rows with and without missing values in target\\ndf_train_ori = df[df[target_col].notnull()].copy()\\ndf_missing = df[df[target_col].isnull()].copy()\\n\\n# Define features (excluding the target column)\\nfeatures = ['humid','rain','temp']\\n\\n# Drop rows with missing values in the features from the training set\\ndf_train = df_train_ori.dropna(subset=features)\\n\\n\\nprint(df_train[features], df_train[target_col])\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train)\n",
    "\n",
    "\"\"\"\n",
    "df = allDf[0].copy()\n",
    "\n",
    "# Choose the column you want to impute (e.g., 'Target')\n",
    "target_col = 'val'\n",
    "\n",
    "# Split data into rows with and without missing values in target\n",
    "df_train_ori = df[df[target_col].notnull()].copy()\n",
    "df_missing = df[df[target_col].isnull()].copy()\n",
    "\n",
    "# Define features (excluding the target column)\n",
    "features = ['humid','rain','temp']\n",
    "\n",
    "# Drop rows with missing values in the features from the training set\n",
    "df_train = df_train_ori.dropna(subset=features)\n",
    "\n",
    "\n",
    "print(df_train[features], df_train[target_col])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train[0], Y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bac3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_missing[features])\n",
    "\n",
    "\"\"\"\n",
    "# Predict missing values\n",
    "df_missing = df_missing.copy()  # to avoid SettingWithCopyWarning\n",
    "df_missing[target_col] = model.predict(df_missing[features])\n",
    "\n",
    "\n",
    "# Combine the imputed rows with the original data\n",
    "df_imputed = pd.concat([df_train_ori, df_missing]).sort_index()\n",
    "\n",
    "print(df_imputed)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feadf814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.69602578e-02  1.29645201e-01  1.96231621e-01 ...  1.10932351e-02\n",
      "  -1.56313027e-05  0.00000000e+00]\n",
      " [ 1.78783914e-01  3.52785848e-01  5.35610258e-01 ...  2.05849756e-01\n",
      "   2.05819015e-01  1.68070251e-01]\n",
      " [ 6.83078874e-01  4.24767083e-01  3.59684220e-01 ... -7.94782331e-03\n",
      "   7.42392609e-03  5.57159891e-01]\n",
      " ...\n",
      " [ 1.62713419e-01  1.27080715e-01  1.18805660e-02 ...  2.70650748e-01\n",
      "   2.88022249e-01  1.58026222e-01]\n",
      " [ 1.92175952e-01  1.93961203e-01  1.54100905e-01 ...  3.42555014e-01\n",
      "   3.57487331e-01  3.61585416e-01]\n",
      " [            nan             nan  1.67072494e-01 ...  3.32941926e-01\n",
      "   3.21561461e-01  3.20070032e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(interpolate_feature(train_dfs, 'val'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e4211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
