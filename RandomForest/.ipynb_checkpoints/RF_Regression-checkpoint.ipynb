{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b92e8735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os \n",
    "import math \n",
    "from scipy.spatial import cKDTree\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.feature as cfeature\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2d4687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_lat_lon(lat, lon, distance, bearing):\n",
    "    lat_rad = math.radians(lat)\n",
    "    lon_rad = math.radians(lon)\n",
    "    angular_distance = distance / EARTH_RADIUS\n",
    "\n",
    "    new_lat = math.asin(math.sin(lat_rad) * math.cos(angular_distance) +\n",
    "                        math.cos(lat_rad) * math.sin(angular_distance) * math.cos(bearing))\n",
    "\n",
    "    new_lon = lon_rad + math.atan2(math.sin(bearing) * math.sin(angular_distance) * math.cos(lat_rad),\n",
    "                                   math.cos(angular_distance) - math.sin(lat_rad) * math.sin(new_lat))\n",
    "\n",
    "    new_lat = math.degrees(new_lat)\n",
    "    new_lon = math.degrees(new_lon)\n",
    "\n",
    "    return new_lat, new_lon\n",
    "\n",
    "# Function to generate the grid points\n",
    "def generate_grid_points(top_left_lat, top_left_lon, bottom_right_lat, bottom_right_lon, grid_size):\n",
    "    grid_points = []\n",
    "\n",
    "    # Calculate the distance between the top-left and bottom-right corners\n",
    "    lat_distance = abs(top_left_lat - bottom_right_lat)\n",
    "    lon_distance = abs(top_left_lon - bottom_right_lon)\n",
    "\n",
    "    # Calculate the number of grids in latitude and longitude directions\n",
    "    num_lat_grids = int(lat_distance * 111.32 / grid_size)  # 1 degree latitude ~ 111.32 km\n",
    "    num_lon_grids = int(lon_distance * 111.32 * math.cos(math.radians(top_left_lat)) / grid_size)\n",
    "\n",
    "    # Generate grid points\n",
    "    for i in range(num_lat_grids + 1):\n",
    "        for j in range(num_lon_grids + 1):\n",
    "            lat = top_left_lat - (i * grid_size / 111.32)\n",
    "            lon = top_left_lon + (j * grid_size / (111.32 * math.cos(math.radians(top_left_lat))))\n",
    "            grid_points.append((lat, lon))\n",
    "\n",
    "    return grid_points\n",
    "\n",
    "def ReadData(csv_file, lat, lon):\n",
    "    # Determine the bounding box of the SMAP data\n",
    "    lat_min, lat_max = min(lat), max(lat)\n",
    "    lon_min, lon_max = min(lon), max(lon)\n",
    "\n",
    "    # Load telemetry station data (CSV format assumed)\n",
    "    #tele_data = pd.read_csv(csv_file, names=['code','latitude','longitude','val'])\n",
    "    tele_data = pd.read_csv(csv_file)\n",
    "    #print(tele_data)\n",
    "\n",
    "    # Filter telemetry stations within SMAP bounding box\n",
    "    filtered_stations = tele_data[\n",
    "        (tele_data['latitude'] >= lat_min) & (tele_data['latitude'] <= lat_max) &\n",
    "        (tele_data['longitude'] >= lon_min) & (tele_data['longitude'] <= lon_max)\n",
    "    ]\n",
    "    return filtered_stations\n",
    "\n",
    "def genSMAP(filtered_stations, smap_locations, _smapDf, paraName):\n",
    "    tele_locations = filtered_stations[['latitude', 'longitude']].to_numpy()\n",
    "    tele_values = filtered_stations['val'].to_numpy()\n",
    "\n",
    "    #print(tele_locations, tele_values)\n",
    "\n",
    "    smap_tree = cKDTree(smap_locations)\n",
    "\n",
    "    # Keep track of used locations in smapDf\n",
    "    used_smap_indices = set()\n",
    "\n",
    "    # Prepare a column to store results\n",
    "    _smapDf[paraName] = np.nan  # New column for matched SMAP values\n",
    "\n",
    "\n",
    "    # Iterate over each smap location and match it to the nearest tele location\n",
    "    for idx, tele_loc in enumerate(tele_locations):\n",
    "        # Query the nearest tele location\n",
    "        #distance, tele_idx = tele_tree.query(tele_loc)\n",
    "        distance, smap_idx = smap_tree.query(tele_loc)\n",
    "        #print(distance, smap_idx,idx , tele_loc, smap_locations[smap_idx])\n",
    "\n",
    "        if smap_idx not in used_smap_indices:\n",
    "            _smapDf.loc[smap_idx, paraName] = tele_values[idx]\n",
    "            #print(distance, smap_idx,idx , tele_loc, smap_locations[smap_idx],tele_values[idx], smapDf['matched_smap_val'][idx])\n",
    "            used_smap_indices.add(smap_idx)  # Mark this SMAP index as used\n",
    "\n",
    "    #print(smapDf)\n",
    "    return _smapDf\n",
    "\n",
    "# Function to interpolate missing values\n",
    "def interpolate_feature(df, feature):\n",
    "    known_points = df[['latitude', 'longitude']][df[feature].notna()].values\n",
    "    known_values = df[feature].dropna().values\n",
    "    grid_values = griddata(known_points, known_values, (grid_lat, grid_lon), method='cubic')\n",
    "    return grid_values\n",
    "\n",
    "def list_files(directory: str, ftype):\n",
    "    \"\"\"\n",
    "    List files all file in given folder.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Directory to search for files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are week ranges and values are lists of matching files.\n",
    "    \"\"\"\n",
    "    matching_files = []\n",
    "    matching_files.extend(\n",
    "        [directory+\"/\"+f for f in os.listdir(directory) if f.endswith(ftype)]\n",
    "    )\n",
    "    #files_by_week[f\"{start} to {stop}\"] = matching_files\n",
    "\n",
    "    return matching_files\n",
    "\n",
    "def load_combined_file(file_path):\n",
    "    \"\"\"\n",
    "    Load the combined HDF5 file and extract the data.\n",
    "    \"\"\"\n",
    "    print(file_path)\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        soil_moisture = f['soil_moisture'][:]\n",
    "        latitude = f['latitude'][:]\n",
    "        longitude = f['longitude'][:]\n",
    "\n",
    "    #print(soil_moisture)\n",
    "\n",
    "    \"\"\"\n",
    "    plot_on_map(\n",
    "        latitude,\n",
    "        longitude,\n",
    "        soil_moisture,\n",
    "        [4.5, 25.5, 95.5, 110.5],\n",
    "        #p_name,\n",
    "        title=\"\"\n",
    "    )\n",
    "    \"\"\"\n",
    "    return soil_moisture, latitude, longitude\n",
    "\n",
    "def generate_points_in_boundary(start_lat, start_lon, end_lat, end_lon, interval_km=1):\n",
    "    \"\"\"\n",
    "    Generate latitude and longitude points at every `interval_km` between two points.\n",
    "    \"\"\"\n",
    "    # Define the starting and ending points\n",
    "    start_point = Point(start_lat, start_lon)\n",
    "    end_point = Point(end_lat, end_lon)\n",
    "\n",
    "    # Calculate the total distance between the start and end points\n",
    "    total_distance = geodesic(start_point, end_point).kilometers\n",
    "\n",
    "    # Calculate the bearing (direction) from start to end\n",
    "    bearing = calculate_bearing(start_point, end_point)\n",
    "\n",
    "    # Generate points along the line at 1 km intervals\n",
    "    pointsDf = pd.DataFrame(columns=['latitude','longitude'])\n",
    "    for km in range(0, int(total_distance), interval_km):\n",
    "        # Calculate the new point at the given distance and bearing\n",
    "        new_point = distance(kilometers=km).destination(point=start_point, bearing=bearing)\n",
    "        pointsDf = pointsDf._append({'latitude': new_point.latitude,\n",
    "                       'longitude': new_point.longitude},\n",
    "                      ignore_index=True)\n",
    "        #points.append((new_point.latitude, new_point.longitude))\n",
    "\n",
    "    return pointsDf\n",
    "\n",
    "def calculate_bearing(start_point, end_point):\n",
    "    \"\"\"\n",
    "    Calculate the bearing (direction) from start_point to end_point.\n",
    "    \"\"\"\n",
    "    lat1 = math.radians(start_point.latitude)\n",
    "    lon1 = math.radians(start_point.longitude)\n",
    "    lat2 = math.radians(end_point.latitude)\n",
    "    lon2 = math.radians(end_point.longitude)\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    x = math.sin(dlon) * math.cos(lat2)\n",
    "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(dlon)\n",
    "    bearing = math.atan2(x, y)\n",
    "    bearing = math.degrees(bearing)\n",
    "    bearing = (bearing + 360) % 360  # Normalize to 0-360 degrees\n",
    "    return bearing\n",
    "\n",
    "def plot_on_map(latitude, longitude, soil_moisture, region_bounds, title):\n",
    "    \"\"\"\n",
    "    Plot soil moisture data on a map using cartopy.\n",
    "    \"\"\"\n",
    "    # Create the map projection\n",
    "    projection = ccrs.PlateCarree()\n",
    "\n",
    "    # Create the figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw={'projection': projection})\n",
    "\n",
    "    # Set the map extent to Thailand\n",
    "    min_lat, max_lat, min_lon, max_lon = region_bounds\n",
    "    ax.set_extent([min_lon, max_lon, min_lat, max_lat], crs=projection)\n",
    "\n",
    "    # Add map features\n",
    "    ax.add_feature(cfeature.LAND, edgecolor='black')\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "\n",
    "\n",
    "    sc = ax.scatter(longitude, latitude, c=soil_moisture, cmap='YlGnBu', marker='s', s=5, transform=projection)\n",
    "    plt.colorbar(sc, ax=ax, orientation='vertical', label='Soil Moisture')\n",
    "\n",
    "    # Add a title\n",
    "    ax.set_title(title, fontsize=14)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f0802655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature1  Feature2  Target\n",
      "0       1.0      10.0   100.0\n",
      "1       2.0      15.0   200.0\n",
      "2       3.0       NaN   300.0\n",
      "4       NaN      25.0   500.0\n",
      "   Feature1  Feature2  Target\n",
      "3       4.0      20.0     NaN\n",
      "Index(['Feature1', 'Feature2'], dtype='object')\n",
      "   Feature1  Feature2  Target\n",
      "0       1.0      10.0   100.0\n",
      "1       2.0      15.0   200.0\n",
      "   Feature1  Feature2  Target\n",
      "0       1.0      10.0   100.0\n",
      "1       2.0      15.0   200.0\n",
      "2       3.0       NaN   300.0\n",
      "3       4.0      20.0   174.0\n",
      "4       NaN      25.0   500.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample data with missing values\n",
    "data = {\n",
    "    'Feature1': [1.0, 2.0, 3.0, 4.0, np.nan],\n",
    "    'Feature2': [10.0, 15.0, np.nan, 20.0, 25.0],\n",
    "    'Target':   [100.0, 200.0, 300.0, np.nan, 500.0]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Choose the column you want to impute (e.g., 'Target')\n",
    "target_col = 'Target'\n",
    "\n",
    "# Split data into rows with and without missing values in target\n",
    "df_train_ori = df[df[target_col].notnull()].copy()\n",
    "df_missing = df[df[target_col].isnull()].copy()\n",
    "\n",
    "print(df_train_ori)\n",
    "print(df_missing)\n",
    "\n",
    "# Define features (excluding the target column)\n",
    "features = df.columns.drop(target_col)\n",
    "print(features)\n",
    "\n",
    "# Drop rows with missing values in the features from the training set\n",
    "df_train = df_train_ori.dropna(subset=features).copy()\n",
    "print(df_train)\n",
    "\n",
    "\n",
    "# Train a RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(df_train[features], df_train[target_col])\n",
    "\n",
    "# Predict missing values\n",
    "df_missing = df_missing.copy()  # to avoid SettingWithCopyWarning\n",
    "df_missing[target_col] = model.predict(df_missing[features])\n",
    "\n",
    "# Combine the imputed rows with the original data\n",
    "df_imputed = pd.concat([df_train_ori, df_missing]).sort_index()\n",
    "\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22a6fe51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         latitude   longitude\n",
      "0       18.607933  101.005346\n",
      "1       18.607933  101.014825\n",
      "2       18.607933  101.024303\n",
      "3       18.607933  101.033782\n",
      "4       18.607933  101.043260\n",
      "...           ...         ...\n",
      "269819  14.017563  105.953182\n",
      "269820  14.017563  105.962661\n",
      "269821  14.017563  105.972139\n",
      "269822  14.017563  105.981618\n",
      "269823  14.017563  105.991097\n",
      "\n",
      "[269824 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "EARTH_RADIUS = 6371  # Earth's radius in kilometers\n",
    "grid_size = 1\n",
    "\n",
    "# NorthEast\n",
    "lat_range = [18.607933, 14.012681]  # Define the latitude range of interest\n",
    "lon_range = [101.005346, 105.995516]  # Define the longitude range of interest\n",
    "\n",
    "# Top Left\n",
    "#lat_range = [18.607933, 16.310307]  # Define the latitude range of interest\n",
    "#lon_range = [101.005346, 103.5004]  # Define the longitude range of interest\n",
    "\n",
    "# Bottom Left\n",
    "#lat_range = [16.310307, 14.012681]  # Define the latitude range of interest\n",
    "#lon_range = [101.005346, 103.5004]  # Define the longitude range of interest\n",
    "\n",
    "# Top Right\n",
    "#lat_range = [18.607933, 16.310307]  # Define the latitude range of interest\n",
    "#lon_range = [103.5004, 105.995516]  # Define the longitude range of interest\n",
    "\n",
    "# Bottom Right\n",
    "#lat_range = [16.310307, 14.012681]  # Define the latitude range of interest\n",
    "#lon_range = [103.5004, 105.995516]  # Define the longitude range of interest\n",
    "\n",
    "# Calculate the distance in kilometers\n",
    "#distance_left_2_right = geodesic((lat_range[0], lon_range[0]), (lat_range[0], lon_range[1])).kilometers\n",
    "#distance_top_2_down = geodesic((lat_range[0], lon_range[0]), (lat_range[1], lon_range[0])).kilometers\n",
    "\n",
    "#print(distance_left_2_right)\n",
    "#print(distance_top_2_down)\n",
    "\n",
    "grid_points = generate_grid_points(lat_range[0], lon_range[0], lat_range[1], lon_range[1], grid_size)\n",
    "\n",
    "points = pd.DataFrame(grid_points, columns=['latitude', 'longitude'])\n",
    "print(points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cace37e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-01-22to2024-01-28.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-11-13to2023-11-19.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-04-17to2023-04-23.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-12-11to2023-12-17.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-04-24to2023-04-30.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-08-05to2024-08-11.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-07-10to2023-07-16.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-03-18to2024-03-24.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-01-02to2023-01-08.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-10-14to2024-10-20.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-10-02to2023-10-08.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-11-04to2024-11-10.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-09-23to2024-09-29.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-01-15to2024-01-21.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-07-24to2023-07-30.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-11-18to2024-11-24.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-04-10to2023-04-16.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-01-08to2024-01-14.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-12-25to2023-12-31.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-05-20to2024-05-26.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-03-13to2023-03-19.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-10-30to2023-11-05.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-10-21to2024-10-27.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-07-17to2023-07-23.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-02-06to2023-02-12.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-05-01to2023-05-07.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-01-30to2023-02-05.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-03-04to2024-03-10.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-07-29to2024-08-04.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-08-19to2024-08-25.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-09-09to2024-09-15.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-10-28to2024-11-03.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-09-11to2023-09-17.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-09-30to2024-10-06.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-07-31to2023-08-06.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-02-05to2024-02-11.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-02-26to2024-03-03.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-07-03to2023-07-09.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-07-15to2024-07-21.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-12-23to2024-12-29.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-07-22to2024-07-28.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-06-12to2023-06-18.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-03-06to2023-03-12.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-04-08to2024-04-14.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-02-13to2023-02-19.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-05-29to2023-06-04.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-04-03to2023-04-09.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-04-15to2024-04-21.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-08-07to2023-08-13.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-01-16to2023-01-22.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-07-08to2024-07-14.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-02-19to2024-02-25.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-12-09to2024-12-15.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-10-16to2023-10-22.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-09-25to2023-10-01.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-11-06to2023-11-12.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-02-27to2023-03-05.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-05-27to2024-06-02.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-04-22to2024-04-28.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-04-01to2024-04-07.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-09-18to2023-09-24.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-08-21to2023-08-27.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-06-05to2023-06-11.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-12-02to2024-12-08.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-11-25to2024-12-01.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-01-29to2024-02-04.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-06-24to2024-06-30.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-11-20to2023-11-26.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-02-12to2024-02-18.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-06-17to2024-06-23.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-09-16to2024-09-22.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-07-01to2024-07-07.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-09-04to2023-09-10.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-08-14to2023-08-20.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-06-26to2023-07-02.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-03-20to2023-03-26.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-06-19to2023-06-25.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-05-13to2024-05-19.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-06-10to2024-06-16.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-04-29to2024-05-05.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-09-02to2024-09-08.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-10-23to2023-10-29.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-03-11to2024-03-17.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-05-06to2024-05-12.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-02-20to2023-02-26.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-10-07to2024-10-13.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-12-18to2023-12-24.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-01-23to2023-01-29.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-05-22to2023-05-28.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-03-27to2023-04-02.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-05-15to2023-05-21.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-08-26to2024-09-01.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-08-12to2024-08-18.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-12-30to2024-12-31.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-03-25to2024-03-31.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-11-11to2024-11-17.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-08-28to2023-09-03.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-05-08to2023-05-14.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-01-09to2023-01-15.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-12-16to2024-12-22.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-10-09to2023-10-15.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-11-27to2023-12-03.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2023-12-04to2023-12-10.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-01-01to2024-01-07.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2022-12-26to2023-01-01.h5', '/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-06-03to2024-06-09.h5']\n"
     ]
    }
   ],
   "source": [
    "# Paths to your data files\n",
    "smap_dir = \"/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand/\"  # Replace with your .h5 file\n",
    "\n",
    "tele_humid_dir = '/Users/khaitao/Documents/GitHub/SMAP/AvgTeleHumid/AvgTeleHumid_'  # Replace with your telemetry data file\n",
    "tele_temp_dir = '/Users/khaitao/Documents/GitHub/SMAP/AvgTeleTemp/AvgTeleTemp_'  # Replace with your telemetry data file\n",
    "tele_rain_dir = '/Users/khaitao/Documents/GitHub/SMAP/AvgTeleRain/AvgTeleRain_'  # Replace with your telemetry data file\n",
    "\n",
    "h5_files = list_files(smap_dir,'.h5')\n",
    "#print(h5_files)\n",
    "\n",
    "\n",
    "h5_files = list_files(smap_dir,'.h5')\n",
    "print(h5_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "872c210a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269824\n",
      "512 527\n",
      "/Users/khaitao/Documents/GitHub/SMAP/Weekly/Thailand//2024-01-22to2024-01-28.h5\n"
     ]
    }
   ],
   "source": [
    "allDf = []\n",
    "TrainList = []\n",
    "TestList = []\n",
    "cc = 0\n",
    "\n",
    "# Build a KDTree for smapDf locations\n",
    "smap_locations = points[['latitude', 'longitude']].to_numpy()\n",
    "print(len(smap_locations))\n",
    "\n",
    "#break\n",
    "\n",
    "lenLat = len(points['latitude'].unique())\n",
    "lenLon = len(points['longitude'].unique())\n",
    "\n",
    "print(lenLat, lenLon)\n",
    "#break\n",
    "\n",
    "#print(grid_lat.shape, grid_lon.shape)\n",
    "\n",
    "#break\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "first_round = True\n",
    "\n",
    "for combined_file in h5_files:\n",
    "    #print(combined_file)\n",
    "    soil_moisture, latitude, longitude = load_combined_file(combined_file)\n",
    "    #print(soil_moisture)\n",
    "\n",
    "    #print(latitude, longitude)\n",
    "    # Mask invalid soil moisture data\n",
    "    soil_moisture = np.ma.masked_invalid(soil_moisture)\n",
    "    p_name = combined_file.replace(smap_dir+\"/\",\"\")\n",
    "    p_name = p_name.replace(\".h5\",\"\")\n",
    "    p_name = p_name[:10].replace(\"-\",\"\")\n",
    "    #print(p_name)\n",
    "\n",
    "    humidDf= ReadData(tele_humid_dir+p_name+\".csv\", latitude, longitude)\n",
    "    humidDf = humidDf.reset_index(drop=True)\n",
    "    #print(humidDf)\n",
    "\n",
    "    tempDf= ReadData(tele_temp_dir+p_name+\".csv\", latitude, longitude)\n",
    "    tempDf = tempDf.reset_index(drop=True)\n",
    "    #print(tempDf)\n",
    "\n",
    "    rainDf= ReadData(tele_rain_dir+p_name+\".csv\", latitude, longitude)\n",
    "    rainDf = rainDf.reset_index(drop=True)\n",
    "    #print(rainDf)\n",
    "\n",
    "    smapDf = pd.DataFrame(columns=['latitude','longitude','val'])\n",
    "    smapDf['latitude'] = latitude\n",
    "    smapDf['longitude'] = longitude\n",
    "    smapDf['val'] = soil_moisture\n",
    "\n",
    "    #print(smapDf[:10])\n",
    "\n",
    "    smapDf = genSMAP(smapDf, smap_locations, points, \"val\")\n",
    "    #print(smapDf[:10])\n",
    "\n",
    "    #break\n",
    "    smapDf = genSMAP(humidDf, smap_locations, smapDf, \"humid\")\n",
    "    #print(smapDf[:10])\n",
    "    smapDf = genSMAP(rainDf, smap_locations, smapDf, \"rain\")\n",
    "    #print(smapDf[:10])\n",
    "    smapDf = genSMAP(tempDf, smap_locations, smapDf, \"temp\")\n",
    "    #print(smapDf[:10])\n",
    "    #print(len(smapDf))\n",
    "\n",
    "    if first_round:\n",
    "        smapDf[['val','humid','rain','temp']] = min_max_scaler.fit_transform(smapDf[['val','humid','rain','temp']])\n",
    "        first_round = False\n",
    "    else:\n",
    "        smapDf[['val','humid','rain','temp']] = min_max_scaler.transform(smapDf[['val','humid','rain','temp']])\n",
    "\n",
    "    allDf.append(smapDf.copy())\n",
    "\n",
    "    #print(f\"{cc}->{allDf}\")\n",
    "\n",
    "    #if cc<=5:\n",
    "    #    cc = cc+1\n",
    "    #else:\n",
    "    #   break\n",
    "\n",
    "    break\n",
    "\n",
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c33c24d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         latitude   longitude       val  humid  rain  temp\n",
      "0       18.607933  101.005346  0.000000    NaN   NaN   NaN\n",
      "1       18.607933  101.014825  0.168070    NaN   NaN   NaN\n",
      "2       18.607933  101.024303  0.557160    NaN   NaN   NaN\n",
      "3       18.607933  101.033782  0.494836    NaN   NaN   NaN\n",
      "4       18.607933  101.043260  0.379273    NaN   NaN   NaN\n",
      "...           ...         ...       ...    ...   ...   ...\n",
      "269819  14.017563  105.953182  0.125216    NaN   NaN   NaN\n",
      "269820  14.017563  105.962661  0.320070    NaN   NaN   NaN\n",
      "269821  14.017563  105.972139  0.162713    NaN   NaN   NaN\n",
      "269822  14.017563  105.981618  0.192176    NaN   NaN   NaN\n",
      "269823  14.017563  105.991097  0.077004    NaN   NaN   NaN\n",
      "\n",
      "[269824 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(allDf[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bcf7932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         latitude   longitude       val  humid  rain  temp\n",
      "0       18.607933  101.005346  0.000000    NaN   NaN   NaN\n",
      "1       18.607933  101.014825  0.168070    NaN   NaN   NaN\n",
      "2       18.607933  101.024303  0.557160    NaN   NaN   NaN\n",
      "3       18.607933  101.033782  0.494836    NaN   NaN   NaN\n",
      "4       18.607933  101.043260  0.379273    NaN   NaN   NaN\n",
      "...           ...         ...       ...    ...   ...   ...\n",
      "269819  14.017563  105.953182  0.125216    NaN   NaN   NaN\n",
      "269820  14.017563  105.962661  0.320070    NaN   NaN   NaN\n",
      "269821  14.017563  105.972139  0.162713    NaN   NaN   NaN\n",
      "269822  14.017563  105.981618  0.192176    NaN   NaN   NaN\n",
      "269823  14.017563  105.991097  0.077004    NaN   NaN   NaN\n",
      "\n",
      "[200650 rows x 6 columns]          latitude   longitude  val  humid  rain  temp\n",
      "11      18.607933  101.109611  NaN    NaN   NaN   NaN\n",
      "22      18.607933  101.213875  NaN    NaN   NaN   NaN\n",
      "34      18.607933  101.327619  NaN    NaN   NaN   NaN\n",
      "45      18.607933  101.431884  NaN    NaN   NaN   NaN\n",
      "57      18.607933  101.545627  NaN    NaN   NaN   NaN\n",
      "...           ...         ...  ...    ...   ...   ...\n",
      "269771  14.017563  105.498209  NaN    NaN   NaN   NaN\n",
      "269783  14.017563  105.611952  NaN    NaN   NaN   NaN\n",
      "269795  14.017563  105.725696  NaN    NaN   NaN   NaN\n",
      "269806  14.017563  105.829960  NaN    NaN   NaN   NaN\n",
      "269818  14.017563  105.943704  NaN    NaN   NaN   NaN\n",
      "\n",
      "[69174 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df = allDf[0].copy()\n",
    "\n",
    "# Choose the column you want to impute (e.g., 'Target')\n",
    "target_col = 'val'\n",
    "\n",
    "# Split data into rows with and without missing values in target\n",
    "df_train_ori = df[df[target_col].notnull()].copy()\n",
    "df_missing = df[df[target_col].isnull()].copy()\n",
    "\n",
    "# Define features (excluding the target column)\n",
    "features = ['humid','rain','temp']\n",
    "\n",
    "# Drop rows with missing values in the features from the training set\n",
    "df_train = df_train_ori.dropna(subset=features)\n",
    "\n",
    "\n",
    "print(df_train_ori, df_missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46eb1891",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Predict missing values\u001b[39;00m\n\u001b[1;32m      6\u001b[0m df_missing \u001b[38;5;241m=\u001b[39m df_missing\u001b[38;5;241m.\u001b[39mcopy()  \u001b[38;5;66;03m# to avoid SettingWithCopyWarning\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df_missing[target_col] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_missing\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/universe_m1/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:991\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    989\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m--> 991\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m    994\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[0;32m~/miniforge3/envs/universe_m1/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:605\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[1;32m    604\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 605\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/universe_m1/lib/python3.9/site-packages/sklearn/base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 577\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/miniforge3/envs/universe_m1/lib/python3.9/site-packages/sklearn/utils/validation.py:895\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    890\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    891\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    892\u001b[0m         )\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 895\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    903\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/miniforge3/envs/universe_m1/lib/python3.9/site-packages/sklearn/utils/validation.py:142\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;129;01mnot\u001b[39;00m allow_nan\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m estimator_name\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m             msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    133\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://scikit-learn.org/stable/modules/impute.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m             )\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html"
     ]
    }
   ],
   "source": [
    "# Train a RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(df_train[features], df_train[target_col])\n",
    "\n",
    "# Predict missing values\n",
    "df_missing = df_missing.copy()  # to avoid SettingWithCopyWarning\n",
    "df_missing[target_col] = model.predict(df_missing[features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4dac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the imputed rows with the original data\n",
    "df_imputed = pd.concat([df_train_ori, df_missing]).sort_index()\n",
    "\n",
    "print(df_imputed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
